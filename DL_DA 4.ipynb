{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtalkGEkmnjNzjEyutSGFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhi2399/deep-learning/blob/main/DL_DA%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iWZBaKhhf0X"
      },
      "source": [
        "#20MAI0020- Nidhi Ghuble"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw5wmeG7m1Sh"
      },
      "source": [
        "#AlexNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR46a3hMcBnc"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zyD2t16Ic-g5",
        "outputId": "53ee6015-d32a-48cc-bea4-c6c4b47fd828"
      },
      "source": [
        "AlexNet = Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(10))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 4, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 2, 2, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-vzZQRUdHVn"
      },
      "source": [
        "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W8W1itUBdc4d",
        "outputId": "f5449430-eb28-45a3-8832-fce7fb734774"
      },
      "source": [
        "#Keras library for CIFAR dataset\n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train),(x_test, y_test)=cifar10.load_data()\n",
        "\n",
        "#Train-validation-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\n",
        "\n",
        "\n",
        "#Dimension of the CIFAR10 dataset\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((35000, 32, 32, 3), (35000, 1))\n",
            "((15000, 32, 32, 3), (15000, 1))\n",
            "((10000, 32, 32, 3), (10000, 1))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RubZ625Edgx8",
        "outputId": "b5109013-e8d1-4d2f-ba4e-08edcd653e59"
      },
      "source": [
        "#Onehot Encoding the labels.\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "#Since we have 10 classes we should expect the shape[1] of y_train,y_val and y_test to change from 1 to 10\n",
        "y_train=to_categorical(y_train)\n",
        "y_val=to_categorical(y_val)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "#Verifying the dimension after one hot encoding\n",
        "print((x_train.shape,y_train.shape))\n",
        "print((x_val.shape,y_val.shape))\n",
        "print((x_test.shape,y_test.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "((35000, 32, 32, 3), (35000, 10))\n",
            "((15000, 32, 32, 3), (15000, 10))\n",
            "((10000, 32, 32, 3), (10000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vlxlqDsodqr4",
        "outputId": "3b2be751-bf38-40a5-c44d-6ff566df5592"
      },
      "source": [
        "#Defining the parameters\n",
        "batch_size= 100\n",
        "epochs=10\n",
        "#Training the model\n",
        "AlexNet.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_val, y_val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "350/350 [==============================] - 13s 23ms/step - loss: 1.8031 - accuracy: 0.3490 - val_loss: 2.2206 - val_accuracy: 0.2805\n",
            "Epoch 2/10\n",
            "350/350 [==============================] - 8s 22ms/step - loss: 1.3877 - accuracy: 0.5149 - val_loss: 1.7963 - val_accuracy: 0.3917\n",
            "Epoch 3/10\n",
            "350/350 [==============================] - 8s 22ms/step - loss: 1.2417 - accuracy: 0.5685 - val_loss: 1.7250 - val_accuracy: 0.4129\n",
            "Epoch 4/10\n",
            "350/350 [==============================] - 8s 22ms/step - loss: 1.1251 - accuracy: 0.6151 - val_loss: 1.9456 - val_accuracy: 0.3595\n",
            "Epoch 5/10\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 1.0125 - accuracy: 0.6543 - val_loss: 1.9490 - val_accuracy: 0.3702\n",
            "Epoch 6/10\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.9166 - accuracy: 0.6951 - val_loss: 1.5616 - val_accuracy: 0.4928\n",
            "Epoch 7/10\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.8198 - accuracy: 0.7285 - val_loss: 1.7675 - val_accuracy: 0.4592\n",
            "Epoch 8/10\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.7371 - accuracy: 0.7564 - val_loss: 2.0326 - val_accuracy: 0.4073\n",
            "Epoch 9/10\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.6398 - accuracy: 0.7918 - val_loss: 1.6689 - val_accuracy: 0.4629\n",
            "Epoch 10/10\n",
            "350/350 [==============================] - 8s 23ms/step - loss: 0.5633 - accuracy: 0.8192 - val_loss: 2.0474 - val_accuracy: 0.4414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb1fb0b3c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o5qSbEdeeQqi",
        "outputId": "55ddc08c-c7eb-4387-ac3d-f4dde81da00f"
      },
      "source": [
        "AlexNet.evaluate(x_test,y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 2.0691 - accuracy: 0.4419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.069119691848755, 0.44190001487731934]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8r5dWoyeqz2"
      },
      "source": [
        "AlexNet.save('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWZuu1XdmPV"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDs6jfqmelb0"
      },
      "source": [
        "Base_model = tf.keras.models.load_model('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmPQg8_HczCu"
      },
      "source": [
        "nb_train_samples =60000\n",
        "nb_valid_samples =10000\n",
        "num_classes = 10"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eGgkgOp3e3Ld",
        "outputId": "6d83a869-19c3-4989-ffe7-fb364ca648bb"
      },
      "source": [
        "(X_train,Y_train), (X_valid, Y_valid) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# expand new axis, channel axis \n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "# [optional]: we may need 3 channel (instead of 1)\n",
        "X_train = np.repeat(X_train, 3, axis=-1)\n",
        "\n",
        "# it's always better to normalize \n",
        "X_train = X_train.astype('float32') / 255\n",
        "\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "X_train = tf.image.resize(X_train, [32,32]) # if we want to resize \n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "X_valid = np.expand_dims(X_valid, axis=-1)\n",
        "\n",
        "# [optional]: we may need 3 channel (instead of 1)\n",
        "X_valid = np.repeat(X_valid, 3, axis=-1)\n",
        "\n",
        "# it's always better to normalize \n",
        "X_valid = X_valid.astype('float32') / 255\n",
        "\n",
        "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
        "X_valid = tf.image.resize(X_valid, [32,32]) # if we want to resize \n",
        "\n",
        "print(X_valid.shape)\n",
        "from keras.utils import np_utils\n",
        "Y_train = np_utils.to_categorical(Y_train[:nb_train_samples], num_classes)\n",
        "Y_valid = np_utils.to_categorical(Y_valid[:nb_valid_samples], num_classes)\n",
        "\n",
        "print((X_train.shape,Y_train.shape))\n",
        "print((X_valid.shape,Y_valid.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(TensorShape([60000, 32, 32, 3]), (60000, 10))\n",
            "(TensorShape([10000, 32, 32, 3]), (10000, 10))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hkNdK5de3eT"
      },
      "source": [
        "from keras.models import load_model\n",
        "new_model = load_model('saved_model/AlexNetModel.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZiLwP1rZfQZG",
        "outputId": "3e058756-92eb-4460-877f-708da7730828"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 8, 8, 96)          34944     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8, 8, 96)          384       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 8, 8, 96)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 96)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 256)         614656    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 2, 2, 384)         885120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 2, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 2, 384)         1536      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10010     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 10)                40        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 25,730,506\n",
            "Trainable params: 25,709,350\n",
            "Non-trainable params: 21,156\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YPMssSqyfWq0",
        "outputId": "29988280-4653-4037-b41e-1bc06ebf8c66"
      },
      "source": [
        "new_model.trainable=False\n",
        "model = tf.keras.Sequential([\n",
        "    new_model,\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 10)                25730506  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 25,730,616\n",
            "Trainable params: 110\n",
            "Non-trainable params: 25,730,506\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iNAU2jhUfh0r",
        "outputId": "acff16c8-eda9-48eb-da6e-c2521c9e1eeb"
      },
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "batch_size=batch_size,\n",
        "epochs=10,\n",
        "verbose=1,\n",
        "validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "600/600 [==============================] - 6s 9ms/step - loss: 2.3059 - accuracy: 0.1038 - val_loss: 2.3008 - val_accuracy: 0.1135\n",
            "Epoch 2/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3017 - accuracy: 0.1113 - val_loss: 2.3007 - val_accuracy: 0.1135\n",
            "Epoch 3/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3017 - accuracy: 0.1143 - val_loss: 2.3008 - val_accuracy: 0.1135\n",
            "Epoch 4/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3017 - accuracy: 0.1110 - val_loss: 2.3006 - val_accuracy: 0.1135\n",
            "Epoch 5/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3014 - accuracy: 0.1122 - val_loss: 2.3006 - val_accuracy: 0.1135\n",
            "Epoch 6/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3010 - accuracy: 0.1130 - val_loss: 2.3007 - val_accuracy: 0.1135\n",
            "Epoch 7/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3008 - accuracy: 0.1134 - val_loss: 2.3007 - val_accuracy: 0.1135\n",
            "Epoch 8/10\n",
            "600/600 [==============================] - 5s 8ms/step - loss: 2.3011 - accuracy: 0.1129 - val_loss: 2.3006 - val_accuracy: 0.1135\n",
            "Epoch 9/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3011 - accuracy: 0.1124 - val_loss: 2.3004 - val_accuracy: 0.1135\n",
            "Epoch 10/10\n",
            "600/600 [==============================] - 5s 9ms/step - loss: 2.3008 - accuracy: 0.1139 - val_loss: 2.3005 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq3yL-C4gCPQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIObbkgVHPQc"
      },
      "source": [
        "#VGG16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "k0hzDhYTGV1v",
        "outputId": "c767555f-7b8f-4b8d-d8eb-4110468aa1e3"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar100\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import optimizers\n",
        "import numpy as np\n",
        "from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "\n",
        "class cifar100vgg:\n",
        "    def __init__(self,train=True):\n",
        "        self.num_classes = 100\n",
        "        self.weight_decay = 0.0005\n",
        "        self.x_shape = [32,32,3]\n",
        "\n",
        "        self.model = self.build_model()\n",
        "        if train:\n",
        "            self.model = self.train(self.model)\n",
        "        else:\n",
        "            self.model.load_weights('cifar100vgg.h5')\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.\n",
        "\n",
        "        model = Sequential()\n",
        "        weight_decay = self.weight_decay\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.3))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "\n",
        "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes))\n",
        "        model.add(Activation('softmax'))\n",
        "        return model\n",
        "\n",
        "\n",
        "    def normalize(self,X_train,X_test):\n",
        "        #this function normalize inputs for zero mean and unit variance\n",
        "        # it is used when training a model.\n",
        "        # Input: training set and test set\n",
        "        # Output: normalized training set and test set according to the trianing set statistics.\n",
        "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        print(mean)\n",
        "        print(std)\n",
        "        X_train = (X_train-mean)/(std+1e-7)\n",
        "        X_test = (X_test-mean)/(std+1e-7)\n",
        "        return X_train, X_test\n",
        "\n",
        "    def normalize_production(self,x):\n",
        "        #this function is used to normalize instances in production according to saved training set statistics\n",
        "        # Input: X - a training set\n",
        "        # Output X - a normalized training set according to normalization constants.\n",
        "\n",
        "        #these values produced during first training and are general for the standard cifar10 training set normalization\n",
        "        mean = 121.936\n",
        "        std = 68.389\n",
        "        return (x-mean)/(std+1e-7)\n",
        "\n",
        "    def predict(self,x,normalize=True,batch_size=50):\n",
        "        if normalize:\n",
        "            x = self.normalize_production(x)\n",
        "        return self.model.predict(x,batch_size)\n",
        "\n",
        "    def train(self,model):\n",
        "\n",
        "        #training parameters\n",
        "        batch_size = 128\n",
        "        maxepoches = 10\n",
        "        learning_rate = 0.1\n",
        "        lr_decay = 1e-6\n",
        "        lr_drop = 20\n",
        "\n",
        "        # The data, shuffled and split between train and test sets:\n",
        "        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train, x_test = self.normalize(x_train, x_test)\n",
        "\n",
        "        y_train = keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "\n",
        "        def lr_scheduler(epoch):\n",
        "            return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "\n",
        "        #data augmentation\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False)  # randomly flip images\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "\n",
        "\n",
        "        #optimization details\n",
        "        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "\n",
        "        # training process in a for loop with learning rate drop every 25 epoches.\n",
        "\n",
        "        historytemp = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                         batch_size=batch_size),\n",
        "                            steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                            epochs=maxepoches,\n",
        "                            validation_data=(x_test, y_test),callbacks=[reduce_lr],verbose=2)\n",
        "        model.save_weights('cifar100vgg.h5')\n",
        "        return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 100)\n",
        "    y_test = keras.utils.to_categorical(y_test, 100)\n",
        "\n",
        "    model = cifar100vgg()\n",
        "\n",
        "    predicted_x = model.predict(x_test)\n",
        "    residuals = (np.argmax(predicted_x,1)!=np.argmax(y_test,1))\n",
        "    loss = sum(residuals)/len(residuals)\n",
        "    print(\"the validation 0/1 loss is: \",loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121.93584\n",
            "68.38902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "390/390 - 31s - loss: 18.8966 - accuracy: 0.0290 - val_loss: 14.5387 - val_accuracy: 0.0129\n",
            "Epoch 2/10\n",
            "390/390 - 28s - loss: 10.9968 - accuracy: 0.0500 - val_loss: 9.3198 - val_accuracy: 0.0258\n",
            "Epoch 3/10\n",
            "390/390 - 28s - loss: 7.2364 - accuracy: 0.0688 - val_loss: 6.6437 - val_accuracy: 0.0343\n",
            "Epoch 4/10\n",
            "390/390 - 28s - loss: 5.5210 - accuracy: 0.0884 - val_loss: 5.2349 - val_accuracy: 0.0760\n",
            "Epoch 5/10\n",
            "390/390 - 28s - loss: 4.7135 - accuracy: 0.1016 - val_loss: 4.5052 - val_accuracy: 0.1138\n",
            "Epoch 6/10\n",
            "390/390 - 29s - loss: 4.3353 - accuracy: 0.1131 - val_loss: 4.1398 - val_accuracy: 0.1308\n",
            "Epoch 7/10\n",
            "390/390 - 29s - loss: 4.0926 - accuracy: 0.1349 - val_loss: 4.0463 - val_accuracy: 0.1394\n",
            "Epoch 8/10\n",
            "390/390 - 29s - loss: 4.0225 - accuracy: 0.1507 - val_loss: 3.8511 - val_accuracy: 0.1776\n",
            "Epoch 9/10\n",
            "390/390 - 29s - loss: 3.8861 - accuracy: 0.1807 - val_loss: 3.7891 - val_accuracy: 0.2163\n",
            "Epoch 10/10\n",
            "390/390 - 29s - loss: 3.7889 - accuracy: 0.2068 - val_loss: 3.8633 - val_accuracy: 0.2069\n",
            "the validation 0/1 loss is:  0.7931\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}